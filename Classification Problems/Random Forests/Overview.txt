# Random forest is an ensemble learning method for classification that takes the majority vote from multiple decision trees to improve accuracy and reduce overfitting.
# It uses bagging (bootstrap aggregating) to create diverse trees by training each tree on a random subset of the data and features.
# Every tree in the forest votes for a class, and the class with the most votes is chosen as the final prediction.
# Every tree is independent, making random forests robust and effective for various classification tasks.

# Implementation
    # Data Preparation
      # The best thing about random forests is that they require very little data preparation. You don't need to scale or normalize your data, and they can handle both numerical and categorical features.
      # The only thing that you need to handle is immbalanced datasets with SMOTE
    # Model Training
        # Grid Search for Hyperparameter Tuning
            # Parameters to tune include:
                # n_estimators: Number of trees in the forest.
                # max_depth: Maximum depth of each tree.
                # min_samples_split: Minimum number of samples required to split a node.
                # min_samples_leaf: Minimum number of samples required at each leaf node.
                # max_features: Number of features to consider when looking for the best split
        # Use the RandomForestClassifier from the sklearn library to create and train a random forest model.
        # Use grid search with cross-validation to find the best hyperparameters for the model.
    # Output
        # After training, evaluate the model's performance using metrics such as accuracy, precision, recall, ROC and F1-score.
        # Use the trained model to make predictions on new data.